{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer() \n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , random_state = 10)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "#### 1 - Define Model\n",
    "\n",
    "- Ensure the input layer has the right number of inputs. This can be specified when creating the first layer with the input_dim argument. \n",
    "\n",
    "- How do we know the number of layers and their types? This is a very hard question. There are heuristics that we can use and often the best network structure is found through a process of trial and error experimentation.\n",
    "\n",
    "- Fully connected layers are defined using the Dense class. We can specify the number of neurons in the layer as the first argument, the initialization method as the second argument as init and specify the activation function using the activation argument.\n",
    "\n",
    "     - uniform distribution ('uniform'): Random number generated between 0 and 0.05\n",
    "     - Normal ('normal'): small random numbers generated from a Gaussian distribution\n",
    "     - We use a sigmoid on the output layer to ensure our network output is between 0 and 1 and easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n",
    "     - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=30, activation='relu'))\n",
    "model.add(Dense(8, activation='relu')) #hidden layer\n",
    "model.add(Dense(1, activation='sigmoid')) #output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "426/426 [==============================] - 11s - loss: 6.0399 - acc: 0.3568       \n",
      "Epoch 2/150\n",
      "426/426 [==============================] - 0s - loss: 1.1022 - acc: 0.5869     \n",
      "Epoch 3/150\n",
      "426/426 [==============================] - 0s - loss: 0.4530 - acc: 0.8263     \n",
      "Epoch 4/150\n",
      "426/426 [==============================] - 0s - loss: 0.3854 - acc: 0.8521     \n",
      "Epoch 5/150\n",
      "426/426 [==============================] - 0s - loss: 0.3374 - acc: 0.8850     \n",
      "Epoch 6/150\n",
      "426/426 [==============================] - 0s - loss: 0.2740 - acc: 0.8897     \n",
      "Epoch 7/150\n",
      "426/426 [==============================] - 0s - loss: 0.2768 - acc: 0.8873     \n",
      "Epoch 8/150\n",
      "426/426 [==============================] - 0s - loss: 0.2518 - acc: 0.8967     \n",
      "Epoch 9/150\n",
      "426/426 [==============================] - 0s - loss: 0.2554 - acc: 0.9038     \n",
      "Epoch 10/150\n",
      "426/426 [==============================] - 0s - loss: 0.2464 - acc: 0.8944     \n",
      "Epoch 11/150\n",
      "426/426 [==============================] - 0s - loss: 0.2322 - acc: 0.9038     \n",
      "Epoch 12/150\n",
      "426/426 [==============================] - 0s - loss: 0.2550 - acc: 0.9038     \n",
      "Epoch 13/150\n",
      "426/426 [==============================] - 0s - loss: 0.2604 - acc: 0.9061     \n",
      "Epoch 14/150\n",
      "426/426 [==============================] - 0s - loss: 0.2390 - acc: 0.9014     \n",
      "Epoch 15/150\n",
      "426/426 [==============================] - 0s - loss: 0.2401 - acc: 0.9085     \n",
      "Epoch 16/150\n",
      "426/426 [==============================] - 0s - loss: 0.2238 - acc: 0.9061     \n",
      "Epoch 17/150\n",
      "426/426 [==============================] - 0s - loss: 0.2878 - acc: 0.8920     \n",
      "Epoch 18/150\n",
      "426/426 [==============================] - 0s - loss: 0.2580 - acc: 0.8944     \n",
      "Epoch 19/150\n",
      "426/426 [==============================] - 0s - loss: 0.2536 - acc: 0.8967     \n",
      "Epoch 20/150\n",
      "426/426 [==============================] - 0s - loss: 0.2170 - acc: 0.9131     \n",
      "Epoch 21/150\n",
      "426/426 [==============================] - 0s - loss: 0.2309 - acc: 0.9061     \n",
      "Epoch 22/150\n",
      "426/426 [==============================] - 0s - loss: 0.2222 - acc: 0.9085     \n",
      "Epoch 23/150\n",
      "426/426 [==============================] - 0s - loss: 0.2267 - acc: 0.9155     \n",
      "Epoch 24/150\n",
      "426/426 [==============================] - 0s - loss: 0.3187 - acc: 0.8897     \n",
      "Epoch 25/150\n",
      "426/426 [==============================] - 0s - loss: 0.2453 - acc: 0.9085     \n",
      "Epoch 26/150\n",
      "426/426 [==============================] - 0s - loss: 0.2388 - acc: 0.9061     \n",
      "Epoch 27/150\n",
      "426/426 [==============================] - 0s - loss: 0.2223 - acc: 0.9038     \n",
      "Epoch 28/150\n",
      "426/426 [==============================] - 0s - loss: 0.2269 - acc: 0.9085     \n",
      "Epoch 29/150\n",
      "426/426 [==============================] - 0s - loss: 0.2090 - acc: 0.9225     \n",
      "Epoch 30/150\n",
      "426/426 [==============================] - 0s - loss: 0.2314 - acc: 0.9131     \n",
      "Epoch 31/150\n",
      "426/426 [==============================] - 0s - loss: 0.2191 - acc: 0.9131     \n",
      "Epoch 32/150\n",
      "426/426 [==============================] - 0s - loss: 0.2237 - acc: 0.8967     \n",
      "Epoch 33/150\n",
      "426/426 [==============================] - 0s - loss: 0.2128 - acc: 0.9131     \n",
      "Epoch 34/150\n",
      "426/426 [==============================] - 0s - loss: 0.2119 - acc: 0.9155     \n",
      "Epoch 35/150\n",
      "426/426 [==============================] - 0s - loss: 0.2058 - acc: 0.9202     \n",
      "Epoch 36/150\n",
      "426/426 [==============================] - 0s - loss: 0.2362 - acc: 0.8920     \n",
      "Epoch 37/150\n",
      "426/426 [==============================] - 0s - loss: 0.2090 - acc: 0.9202     \n",
      "Epoch 38/150\n",
      "426/426 [==============================] - 0s - loss: 0.2064 - acc: 0.9249     \n",
      "Epoch 39/150\n",
      "426/426 [==============================] - 0s - loss: 0.2076 - acc: 0.9085     \n",
      "Epoch 40/150\n",
      "426/426 [==============================] - 0s - loss: 0.2256 - acc: 0.9014     \n",
      "Epoch 41/150\n",
      "426/426 [==============================] - 0s - loss: 0.1899 - acc: 0.9225     \n",
      "Epoch 42/150\n",
      "426/426 [==============================] - 0s - loss: 0.2162 - acc: 0.9272     \n",
      "Epoch 43/150\n",
      "426/426 [==============================] - 0s - loss: 0.2108 - acc: 0.9108     \n",
      "Epoch 44/150\n",
      "426/426 [==============================] - 0s - loss: 0.2272 - acc: 0.9108     \n",
      "Epoch 45/150\n",
      "426/426 [==============================] - 0s - loss: 0.1969 - acc: 0.9225     \n",
      "Epoch 46/150\n",
      "426/426 [==============================] - 0s - loss: 0.1942 - acc: 0.9249     \n",
      "Epoch 47/150\n",
      "426/426 [==============================] - 0s - loss: 0.2303 - acc: 0.9038     \n",
      "Epoch 48/150\n",
      "426/426 [==============================] - 0s - loss: 0.2017 - acc: 0.9225     \n",
      "Epoch 49/150\n",
      "426/426 [==============================] - 0s - loss: 0.1939 - acc: 0.9319     \n",
      "Epoch 50/150\n",
      "426/426 [==============================] - 0s - loss: 0.2329 - acc: 0.9038     \n",
      "Epoch 51/150\n",
      "426/426 [==============================] - 0s - loss: 0.2511 - acc: 0.9108     \n",
      "Epoch 52/150\n",
      "426/426 [==============================] - 0s - loss: 0.1932 - acc: 0.9225     \n",
      "Epoch 53/150\n",
      "426/426 [==============================] - 0s - loss: 0.2276 - acc: 0.9131     \n",
      "Epoch 54/150\n",
      "426/426 [==============================] - 0s - loss: 0.1796 - acc: 0.9272     \n",
      "Epoch 55/150\n",
      "426/426 [==============================] - 0s - loss: 0.1902 - acc: 0.9155     \n",
      "Epoch 56/150\n",
      "426/426 [==============================] - 0s - loss: 0.3278 - acc: 0.8732     \n",
      "Epoch 57/150\n",
      "426/426 [==============================] - 0s - loss: 0.2056 - acc: 0.9155     \n",
      "Epoch 58/150\n",
      "426/426 [==============================] - 0s - loss: 0.1736 - acc: 0.9225     \n",
      "Epoch 59/150\n",
      "426/426 [==============================] - 0s - loss: 0.1760 - acc: 0.9272     \n",
      "Epoch 60/150\n",
      "426/426 [==============================] - 0s - loss: 0.1839 - acc: 0.9272     \n",
      "Epoch 61/150\n",
      "426/426 [==============================] - 0s - loss: 0.2173 - acc: 0.9108     \n",
      "Epoch 62/150\n",
      "426/426 [==============================] - 0s - loss: 0.2066 - acc: 0.9155     \n",
      "Epoch 63/150\n",
      "426/426 [==============================] - 0s - loss: 0.2738 - acc: 0.9061     \n",
      "Epoch 64/150\n",
      "426/426 [==============================] - 0s - loss: 0.2534 - acc: 0.9061     \n",
      "Epoch 65/150\n",
      "426/426 [==============================] - 0s - loss: 0.1996 - acc: 0.9249     \n",
      "Epoch 66/150\n",
      "426/426 [==============================] - 0s - loss: 0.2225 - acc: 0.9155     \n",
      "Epoch 67/150\n",
      "426/426 [==============================] - 0s - loss: 0.2017 - acc: 0.9155     \n",
      "Epoch 68/150\n",
      "426/426 [==============================] - 0s - loss: 0.1686 - acc: 0.9296     \n",
      "Epoch 69/150\n",
      "426/426 [==============================] - 0s - loss: 0.2348 - acc: 0.9108     \n",
      "Epoch 70/150\n",
      "426/426 [==============================] - 0s - loss: 0.1685 - acc: 0.9249     \n",
      "Epoch 71/150\n",
      "426/426 [==============================] - 0s - loss: 0.1739 - acc: 0.9155     \n",
      "Epoch 72/150\n",
      "426/426 [==============================] - 0s - loss: 0.2223 - acc: 0.9085     \n",
      "Epoch 73/150\n",
      "426/426 [==============================] - 0s - loss: 0.1762 - acc: 0.9296     \n",
      "Epoch 74/150\n",
      "426/426 [==============================] - 0s - loss: 0.2044 - acc: 0.9108     \n",
      "Epoch 75/150\n",
      "426/426 [==============================] - 0s - loss: 0.1730 - acc: 0.9178     \n",
      "Epoch 76/150\n",
      "426/426 [==============================] - 0s - loss: 0.1679 - acc: 0.9225     \n",
      "Epoch 77/150\n",
      "426/426 [==============================] - 0s - loss: 0.1815 - acc: 0.9178     \n",
      "Epoch 78/150\n",
      "426/426 [==============================] - 0s - loss: 0.1824 - acc: 0.9202     \n",
      "Epoch 79/150\n",
      "426/426 [==============================] - 0s - loss: 0.1837 - acc: 0.9272     \n",
      "Epoch 80/150\n",
      "426/426 [==============================] - 0s - loss: 0.1751 - acc: 0.9272     \n",
      "Epoch 81/150\n",
      "426/426 [==============================] - 0s - loss: 0.1767 - acc: 0.9202     \n",
      "Epoch 82/150\n",
      "426/426 [==============================] - 0s - loss: 0.1593 - acc: 0.9319     \n",
      "Epoch 83/150\n",
      "426/426 [==============================] - 0s - loss: 0.1545 - acc: 0.9390     \n",
      "Epoch 84/150\n",
      "426/426 [==============================] - 0s - loss: 0.2005 - acc: 0.9225     \n",
      "Epoch 85/150\n",
      "426/426 [==============================] - 0s - loss: 0.1924 - acc: 0.9178     \n",
      "Epoch 86/150\n",
      "426/426 [==============================] - 0s - loss: 0.1616 - acc: 0.9319     \n",
      "Epoch 87/150\n",
      "426/426 [==============================] - 0s - loss: 0.1678 - acc: 0.9390     \n",
      "Epoch 88/150\n",
      "426/426 [==============================] - 0s - loss: 0.1626 - acc: 0.9413     \n",
      "Epoch 89/150\n",
      "426/426 [==============================] - 0s - loss: 0.1557 - acc: 0.9272     \n",
      "Epoch 90/150\n",
      "426/426 [==============================] - 0s - loss: 0.1976 - acc: 0.9108     \n",
      "Epoch 91/150\n",
      "426/426 [==============================] - 0s - loss: 0.1830 - acc: 0.9131     \n",
      "Epoch 92/150\n",
      "426/426 [==============================] - 0s - loss: 0.1715 - acc: 0.9249     \n",
      "Epoch 93/150\n",
      "426/426 [==============================] - 0s - loss: 0.1766 - acc: 0.9272     \n",
      "Epoch 94/150\n",
      "426/426 [==============================] - 0s - loss: 0.2009 - acc: 0.9061     \n",
      "Epoch 95/150\n",
      "426/426 [==============================] - 0s - loss: 0.1771 - acc: 0.9272     \n",
      "Epoch 96/150\n",
      "426/426 [==============================] - 0s - loss: 0.1584 - acc: 0.9272     \n",
      "Epoch 97/150\n",
      "426/426 [==============================] - 0s - loss: 0.1607 - acc: 0.9319     \n",
      "Epoch 98/150\n",
      "426/426 [==============================] - 0s - loss: 0.1536 - acc: 0.9343     \n",
      "Epoch 99/150\n",
      "426/426 [==============================] - 0s - loss: 0.1588 - acc: 0.9272     \n",
      "Epoch 100/150\n",
      "426/426 [==============================] - 0s - loss: 0.1638 - acc: 0.9296     \n",
      "Epoch 101/150\n",
      "426/426 [==============================] - 0s - loss: 0.2202 - acc: 0.9202     \n",
      "Epoch 102/150\n",
      "426/426 [==============================] - 0s - loss: 0.1686 - acc: 0.9319     \n",
      "Epoch 103/150\n",
      "426/426 [==============================] - 0s - loss: 0.1474 - acc: 0.9413     \n",
      "Epoch 104/150\n",
      "426/426 [==============================] - 0s - loss: 0.1624 - acc: 0.9296     \n",
      "Epoch 105/150\n",
      "426/426 [==============================] - 0s - loss: 0.1967 - acc: 0.9178     \n",
      "Epoch 106/150\n",
      "426/426 [==============================] - 0s - loss: 0.1731 - acc: 0.9178     \n",
      "Epoch 107/150\n",
      "426/426 [==============================] - 0s - loss: 0.1589 - acc: 0.9390     \n",
      "Epoch 108/150\n",
      "426/426 [==============================] - 0s - loss: 0.1529 - acc: 0.9390     \n",
      "Epoch 109/150\n",
      "426/426 [==============================] - 0s - loss: 0.1523 - acc: 0.9319     \n",
      "Epoch 110/150\n",
      "426/426 [==============================] - 0s - loss: 0.1726 - acc: 0.9202     \n",
      "Epoch 111/150\n",
      "426/426 [==============================] - 0s - loss: 0.2000 - acc: 0.9249     \n",
      "Epoch 112/150\n",
      "426/426 [==============================] - 0s - loss: 0.1565 - acc: 0.9319     \n",
      "Epoch 113/150\n",
      "426/426 [==============================] - 0s - loss: 0.1679 - acc: 0.9272     \n",
      "Epoch 114/150\n",
      "426/426 [==============================] - 0s - loss: 0.1919 - acc: 0.9343     \n",
      "Epoch 115/150\n",
      "426/426 [==============================] - 0s - loss: 0.1441 - acc: 0.9343     \n",
      "Epoch 116/150\n",
      "426/426 [==============================] - 0s - loss: 0.1718 - acc: 0.9296     \n",
      "Epoch 117/150\n",
      "426/426 [==============================] - 0s - loss: 0.1483 - acc: 0.9319     \n",
      "Epoch 118/150\n",
      "426/426 [==============================] - 0s - loss: 0.1907 - acc: 0.9202     \n",
      "Epoch 119/150\n",
      "426/426 [==============================] - 0s - loss: 0.1600 - acc: 0.9366     \n",
      "Epoch 120/150\n",
      "426/426 [==============================] - 0s - loss: 0.1740 - acc: 0.9319     \n",
      "Epoch 121/150\n",
      "426/426 [==============================] - 0s - loss: 0.1516 - acc: 0.9413     \n",
      "Epoch 122/150\n",
      "426/426 [==============================] - 0s - loss: 0.1608 - acc: 0.9296     \n",
      "Epoch 123/150\n",
      "426/426 [==============================] - 0s - loss: 0.1516 - acc: 0.9413     \n",
      "Epoch 124/150\n",
      "426/426 [==============================] - 0s - loss: 0.1352 - acc: 0.9484     \n",
      "Epoch 125/150\n",
      "426/426 [==============================] - 0s - loss: 0.1636 - acc: 0.9272     \n",
      "Epoch 126/150\n",
      "426/426 [==============================] - 0s - loss: 0.1620 - acc: 0.9460     \n",
      "Epoch 127/150\n",
      "426/426 [==============================] - 0s - loss: 0.1415 - acc: 0.9390     \n",
      "Epoch 128/150\n",
      "426/426 [==============================] - 0s - loss: 0.1635 - acc: 0.9319     \n",
      "Epoch 129/150\n",
      "426/426 [==============================] - 0s - loss: 0.1825 - acc: 0.9272     \n",
      "Epoch 130/150\n",
      "426/426 [==============================] - 0s - loss: 0.1555 - acc: 0.9155     \n",
      "Epoch 131/150\n",
      "426/426 [==============================] - 0s - loss: 0.1408 - acc: 0.9390     \n",
      "Epoch 132/150\n",
      "426/426 [==============================] - 0s - loss: 0.1594 - acc: 0.9343     \n",
      "Epoch 133/150\n",
      "426/426 [==============================] - 0s - loss: 0.1429 - acc: 0.9413     \n",
      "Epoch 134/150\n",
      "426/426 [==============================] - 0s - loss: 0.1662 - acc: 0.9202     \n",
      "Epoch 135/150\n",
      "426/426 [==============================] - 0s - loss: 0.1442 - acc: 0.9366     \n",
      "Epoch 136/150\n",
      "426/426 [==============================] - 0s - loss: 0.1527 - acc: 0.9272     \n",
      "Epoch 137/150\n",
      "426/426 [==============================] - 0s - loss: 0.1569 - acc: 0.9319     \n",
      "Epoch 138/150\n",
      "426/426 [==============================] - 0s - loss: 0.1331 - acc: 0.9437     \n",
      "Epoch 139/150\n",
      "426/426 [==============================] - 0s - loss: 0.1487 - acc: 0.9413     \n",
      "Epoch 140/150\n",
      "426/426 [==============================] - 0s - loss: 0.1473 - acc: 0.9413     \n",
      "Epoch 141/150\n",
      "426/426 [==============================] - 0s - loss: 0.1441 - acc: 0.9413     \n",
      "Epoch 142/150\n",
      "426/426 [==============================] - 0s - loss: 0.1792 - acc: 0.9225     \n",
      "Epoch 143/150\n",
      "426/426 [==============================] - 0s - loss: 0.1455 - acc: 0.9366     \n",
      "Epoch 144/150\n",
      "426/426 [==============================] - 0s - loss: 0.1295 - acc: 0.9413     \n",
      "Epoch 145/150\n",
      "426/426 [==============================] - 0s - loss: 0.1415 - acc: 0.9366     \n",
      "Epoch 146/150\n",
      "426/426 [==============================] - 0s - loss: 0.1400 - acc: 0.9460     \n",
      "Epoch 147/150\n",
      "426/426 [==============================] - 0s - loss: 0.1348 - acc: 0.9437     \n",
      "Epoch 148/150\n",
      "426/426 [==============================] - 0s - loss: 0.1315 - acc: 0.9484     \n",
      "Epoch 149/150\n",
      "426/426 [==============================] - 0s - loss: 0.1512 - acc: 0.9319     \n",
      "Epoch 150/150\n",
      "426/426 [==============================] - 0s - loss: 0.1428 - acc: 0.9366     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2ef46860>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/143 [=====>........................] - ETA: 0s\n",
      "acc: 93.01%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores = model.evaluate(X_test, y_test)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5- Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.43728561e-03],\n",
       "       [  9.77244377e-01],\n",
       "       [  9.93155003e-01],\n",
       "       [  1.98008929e-04],\n",
       "       [  9.85968471e-01],\n",
       "       [  8.39766264e-01],\n",
       "       [  9.93550062e-01],\n",
       "       [  1.34150580e-01],\n",
       "       [  3.72920937e-07],\n",
       "       [  3.32474172e-01],\n",
       "       [  9.96353865e-01],\n",
       "       [  9.97930288e-01],\n",
       "       [  1.97195366e-01],\n",
       "       [  9.90390539e-01],\n",
       "       [  1.02381909e-03],\n",
       "       [  2.42404221e-03],\n",
       "       [  9.90501523e-01],\n",
       "       [  7.50293076e-01],\n",
       "       [  9.66648102e-01],\n",
       "       [  3.33946496e-02],\n",
       "       [  9.44254249e-02],\n",
       "       [  9.41332459e-01],\n",
       "       [  9.49973941e-01],\n",
       "       [  9.76996362e-01],\n",
       "       [  9.89667903e-09],\n",
       "       [  1.82667463e-07],\n",
       "       [  7.71288455e-01],\n",
       "       [  7.65767879e-07],\n",
       "       [  9.93574083e-01],\n",
       "       [  6.81857765e-01],\n",
       "       [  9.86443281e-01],\n",
       "       [  9.85591769e-01],\n",
       "       [  6.95814850e-10],\n",
       "       [  9.96742785e-01],\n",
       "       [  8.27918470e-01],\n",
       "       [  9.79771435e-01],\n",
       "       [  9.10076082e-01],\n",
       "       [  9.94928122e-01],\n",
       "       [  9.94493246e-01],\n",
       "       [  9.61646616e-01],\n",
       "       [  9.94599938e-01],\n",
       "       [  9.95106339e-01],\n",
       "       [  9.40226197e-01],\n",
       "       [  8.37828279e-01],\n",
       "       [  9.90877092e-01],\n",
       "       [  1.03767148e-16],\n",
       "       [  3.82594066e-03],\n",
       "       [  3.93887237e-03],\n",
       "       [  5.45933424e-03],\n",
       "       [  8.18194807e-01],\n",
       "       [  9.21620488e-01],\n",
       "       [  9.74969685e-01],\n",
       "       [  9.45797947e-05],\n",
       "       [  9.90431249e-01],\n",
       "       [  9.65302289e-01],\n",
       "       [  1.02632155e-24],\n",
       "       [  7.58122405e-05],\n",
       "       [  9.96698320e-01],\n",
       "       [  8.44661355e-01],\n",
       "       [  9.89936471e-01],\n",
       "       [  1.57609891e-29],\n",
       "       [  9.91956592e-01],\n",
       "       [  1.83571745e-02],\n",
       "       [  8.22742462e-01],\n",
       "       [  9.97369647e-01],\n",
       "       [  2.50237703e-01],\n",
       "       [  9.02383327e-01],\n",
       "       [  9.99239683e-01],\n",
       "       [  1.39467255e-03],\n",
       "       [  1.44278243e-01],\n",
       "       [  9.98507082e-01],\n",
       "       [  9.92768884e-01],\n",
       "       [  9.88899708e-01],\n",
       "       [  9.89137471e-01],\n",
       "       [  9.79915082e-01],\n",
       "       [  1.68041006e-01],\n",
       "       [  9.89516556e-01],\n",
       "       [  9.61494505e-01],\n",
       "       [  8.73291065e-05],\n",
       "       [  8.64174101e-04],\n",
       "       [  9.95474160e-01],\n",
       "       [  9.76331353e-01],\n",
       "       [  1.15542684e-06],\n",
       "       [  9.67624962e-01],\n",
       "       [  9.96181726e-01],\n",
       "       [  9.87074733e-01],\n",
       "       [  8.78124833e-01],\n",
       "       [  2.08531737e-01],\n",
       "       [  9.84968007e-01],\n",
       "       [  2.34125212e-01],\n",
       "       [  9.88087177e-01],\n",
       "       [  7.92961717e-01],\n",
       "       [  3.08450581e-05],\n",
       "       [  9.46617067e-01],\n",
       "       [  9.96620059e-01],\n",
       "       [  9.92635548e-01],\n",
       "       [  3.59226487e-06],\n",
       "       [  9.82022941e-01],\n",
       "       [  1.41451713e-08],\n",
       "       [  9.86388624e-01],\n",
       "       [  2.39954346e-07],\n",
       "       [  2.56038131e-03],\n",
       "       [  9.85222816e-01],\n",
       "       [  9.97896552e-01],\n",
       "       [  9.96997714e-01],\n",
       "       [  9.98838246e-01],\n",
       "       [  4.31326157e-08],\n",
       "       [  2.83289070e-23],\n",
       "       [  9.96198237e-01],\n",
       "       [  9.93544698e-01],\n",
       "       [  9.89722967e-01],\n",
       "       [  9.46957707e-01],\n",
       "       [  2.46045666e-11],\n",
       "       [  6.52908857e-05],\n",
       "       [  1.72509407e-09],\n",
       "       [  9.95823979e-01],\n",
       "       [  7.36458182e-01],\n",
       "       [  3.47553583e-11],\n",
       "       [  3.97179602e-03],\n",
       "       [  9.94640827e-01],\n",
       "       [  8.34671080e-01],\n",
       "       [  5.10768643e-07],\n",
       "       [  9.93368328e-01],\n",
       "       [  7.12219116e-14],\n",
       "       [  9.97753084e-01],\n",
       "       [  9.46299493e-01],\n",
       "       [  9.16845159e-12],\n",
       "       [  3.09327175e-03],\n",
       "       [  9.90462840e-01],\n",
       "       [  9.82082307e-01],\n",
       "       [  7.75713384e-01],\n",
       "       [  9.95570362e-01],\n",
       "       [  1.66539792e-02],\n",
       "       [  9.97916043e-01],\n",
       "       [  9.94388044e-01],\n",
       "       [  8.97095660e-08],\n",
       "       [  2.58456528e-01],\n",
       "       [  5.87555348e-07],\n",
       "       [  5.29617250e-01],\n",
       "       [  2.02388528e-05],\n",
       "       [  9.56674278e-01],\n",
       "       [  9.97944653e-01],\n",
       "       [  5.23658807e-15]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    #create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_dim=30, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu')) #hidden layer\n",
    "    model.add(Dense(1,activation='sigmoid')) #output layer\n",
    "    #compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "    \n",
    "param_grid = {'epochs':[50, 100, 200] , 'batch_size':[5,10,20, 50, 100]}\n",
    "\n",
    "keras_classifier = KerasClassifier(build_fn = create_model , verbose = 0)\n",
    "\n",
    "grid_search = GridSearchCV(keras_classifier , param_grid , cv =10 )\n",
    "\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "data = load_boston()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Sequential()\n",
    "model.add(Dense(13, input_dim=13, kernel_initializer='normal', activation='relu'))\n",
    "model.add(Dense(6, kernel_initializer = 'normal', activation = 'relu')) # hidden layer\n",
    "model.add(Dense(10, kernel_initializer = 'normal', activation = 'linear'))\n",
    "model.add(Dense(1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2- Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='mse', optimizer='adam' , metrics = ['mse','mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "379/379 [==============================] - 0s 1ms/step - loss: 605.6622 - mean_squared_error: 605.6622 - mean_absolute_error: 22.8186\n",
      "Epoch 2/100\n",
      "379/379 [==============================] - 0s 84us/step - loss: 557.5308 - mean_squared_error: 557.5308 - mean_absolute_error: 21.7308\n",
      "Epoch 3/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 366.2156 - mean_squared_error: 366.2156 - mean_absolute_error: 16.6359\n",
      "Epoch 4/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 127.5395 - mean_squared_error: 127.5395 - mean_absolute_error: 8.6053\n",
      "Epoch 5/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 106.3710 - mean_squared_error: 106.3710 - mean_absolute_error: 7.7680\n",
      "Epoch 6/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 97.5155 - mean_squared_error: 97.5155 - mean_absolute_error: 7.2562\n",
      "Epoch 7/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 92.6192 - mean_squared_error: 92.6192 - mean_absolute_error: 7.1525\n",
      "Epoch 8/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 85.5010 - mean_squared_error: 85.5010 - mean_absolute_error: 6.7731\n",
      "Epoch 9/100\n",
      "379/379 [==============================] - 0s 79us/step - loss: 81.6067 - mean_squared_error: 81.6067 - mean_absolute_error: 6.6604\n",
      "Epoch 10/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 77.8157 - mean_squared_error: 77.8157 - mean_absolute_error: 6.3669\n",
      "Epoch 11/100\n",
      "379/379 [==============================] - 0s 87us/step - loss: 74.3244 - mean_squared_error: 74.3244 - mean_absolute_error: 6.2582\n",
      "Epoch 12/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 72.4347 - mean_squared_error: 72.4347 - mean_absolute_error: 6.1607\n",
      "Epoch 13/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 68.9581 - mean_squared_error: 68.9581 - mean_absolute_error: 6.0169\n",
      "Epoch 14/100\n",
      "379/379 [==============================] - 0s 108us/step - loss: 67.3123 - mean_squared_error: 67.3123 - mean_absolute_error: 5.8682\n",
      "Epoch 15/100\n",
      "379/379 [==============================] - 0s 103us/step - loss: 66.4607 - mean_squared_error: 66.4607 - mean_absolute_error: 6.0114\n",
      "Epoch 16/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 66.2045 - mean_squared_error: 66.2045 - mean_absolute_error: 5.8034\n",
      "Epoch 17/100\n",
      "379/379 [==============================] - 0s 66us/step - loss: 64.9316 - mean_squared_error: 64.9316 - mean_absolute_error: 5.8927\n",
      "Epoch 18/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 64.6095 - mean_squared_error: 64.6095 - mean_absolute_error: 5.8022\n",
      "Epoch 19/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 63.5192 - mean_squared_error: 63.5192 - mean_absolute_error: 5.7565\n",
      "Epoch 20/100\n",
      "379/379 [==============================] - 0s 61us/step - loss: 63.4211 - mean_squared_error: 63.4211 - mean_absolute_error: 5.7929\n",
      "Epoch 21/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 62.8144 - mean_squared_error: 62.8144 - mean_absolute_error: 5.6426\n",
      "Epoch 22/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 64.3468 - mean_squared_error: 64.3468 - mean_absolute_error: 5.7843\n",
      "Epoch 23/100\n",
      "379/379 [==============================] - 0s 63us/step - loss: 62.1115 - mean_squared_error: 62.1115 - mean_absolute_error: 5.7091\n",
      "Epoch 24/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 62.1239 - mean_squared_error: 62.1239 - mean_absolute_error: 5.6032\n",
      "Epoch 25/100\n",
      "379/379 [==============================] - 0s 63us/step - loss: 60.9664 - mean_squared_error: 60.9664 - mean_absolute_error: 5.6276\n",
      "Epoch 26/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 59.9717 - mean_squared_error: 59.9717 - mean_absolute_error: 5.6328\n",
      "Epoch 27/100\n",
      "379/379 [==============================] - 0s 58us/step - loss: 59.9938 - mean_squared_error: 59.9938 - mean_absolute_error: 5.5648\n",
      "Epoch 28/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 59.5148 - mean_squared_error: 59.5148 - mean_absolute_error: 5.6237\n",
      "Epoch 29/100\n",
      "379/379 [==============================] - 0s 69us/step - loss: 59.0066 - mean_squared_error: 59.0066 - mean_absolute_error: 5.4212\n",
      "Epoch 30/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 57.9595 - mean_squared_error: 57.9595 - mean_absolute_error: 5.4756\n",
      "Epoch 31/100\n",
      "379/379 [==============================] - 0s 63us/step - loss: 57.6184 - mean_squared_error: 57.6184 - mean_absolute_error: 5.3938\n",
      "Epoch 32/100\n",
      "379/379 [==============================] - 0s 87us/step - loss: 57.4698 - mean_squared_error: 57.4698 - mean_absolute_error: 5.4712\n",
      "Epoch 33/100\n",
      "379/379 [==============================] - 0s 61us/step - loss: 56.7151 - mean_squared_error: 56.7151 - mean_absolute_error: 5.3020\n",
      "Epoch 34/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 55.6023 - mean_squared_error: 55.6023 - mean_absolute_error: 5.3318\n",
      "Epoch 35/100\n",
      "379/379 [==============================] - 0s 61us/step - loss: 55.4443 - mean_squared_error: 55.4443 - mean_absolute_error: 5.2855\n",
      "Epoch 36/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 55.0859 - mean_squared_error: 55.0859 - mean_absolute_error: 5.3790\n",
      "Epoch 37/100\n",
      "379/379 [==============================] - 0s 63us/step - loss: 53.1565 - mean_squared_error: 53.1565 - mean_absolute_error: 5.1329\n",
      "Epoch 38/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 53.2374 - mean_squared_error: 53.2374 - mean_absolute_error: 5.4014\n",
      "Epoch 39/100\n",
      "379/379 [==============================] - 0s 66us/step - loss: 53.3826 - mean_squared_error: 53.3826 - mean_absolute_error: 5.1333\n",
      "Epoch 40/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 53.3133 - mean_squared_error: 53.3133 - mean_absolute_error: 5.2699\n",
      "Epoch 41/100\n",
      "379/379 [==============================] - 0s 63us/step - loss: 50.9179 - mean_squared_error: 50.9179 - mean_absolute_error: 5.1104\n",
      "Epoch 42/100\n",
      "379/379 [==============================] - 0s 61us/step - loss: 51.0340 - mean_squared_error: 51.0340 - mean_absolute_error: 5.1157\n",
      "Epoch 43/100\n",
      "379/379 [==============================] - 0s 87us/step - loss: 51.5578 - mean_squared_error: 51.5578 - mean_absolute_error: 5.1552\n",
      "Epoch 44/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 50.1118 - mean_squared_error: 50.1118 - mean_absolute_error: 5.0832\n",
      "Epoch 45/100\n",
      "379/379 [==============================] - 0s 100us/step - loss: 48.5066 - mean_squared_error: 48.5066 - mean_absolute_error: 5.0767\n",
      "Epoch 46/100\n",
      "379/379 [==============================] - 0s 61us/step - loss: 46.8986 - mean_squared_error: 46.8986 - mean_absolute_error: 4.8425\n",
      "Epoch 47/100\n",
      "379/379 [==============================] - 0s 87us/step - loss: 47.1260 - mean_squared_error: 47.1260 - mean_absolute_error: 5.0391\n",
      "Epoch 48/100\n",
      "379/379 [==============================] - 0s 66us/step - loss: 46.2191 - mean_squared_error: 46.2191 - mean_absolute_error: 4.9703\n",
      "Epoch 49/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 46.0411 - mean_squared_error: 46.0411 - mean_absolute_error: 4.8428\n",
      "Epoch 50/100\n",
      "379/379 [==============================] - 0s 63us/step - loss: 44.4447 - mean_squared_error: 44.4447 - mean_absolute_error: 4.7569\n",
      "Epoch 51/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 43.4892 - mean_squared_error: 43.4892 - mean_absolute_error: 4.7302\n",
      "Epoch 52/100\n",
      "379/379 [==============================] - 0s 66us/step - loss: 43.5850 - mean_squared_error: 43.5850 - mean_absolute_error: 4.7856\n",
      "Epoch 53/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 42.6767 - mean_squared_error: 42.6767 - mean_absolute_error: 4.7524\n",
      "Epoch 54/100\n",
      "379/379 [==============================] - 0s 77us/step - loss: 41.4822 - mean_squared_error: 41.4822 - mean_absolute_error: 4.5974\n",
      "Epoch 55/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 40.7593 - mean_squared_error: 40.7593 - mean_absolute_error: 4.6488\n",
      "Epoch 56/100\n",
      "379/379 [==============================] - 0s 82us/step - loss: 39.9994 - mean_squared_error: 39.9994 - mean_absolute_error: 4.6238\n",
      "Epoch 57/100\n",
      "379/379 [==============================] - 0s 77us/step - loss: 40.1973 - mean_squared_error: 40.1973 - mean_absolute_error: 4.5648\n",
      "Epoch 58/100\n",
      "379/379 [==============================] - 0s 100us/step - loss: 38.9459 - mean_squared_error: 38.9459 - mean_absolute_error: 4.5471\n",
      "Epoch 59/100\n",
      "379/379 [==============================] - 0s 69us/step - loss: 38.6874 - mean_squared_error: 38.6874 - mean_absolute_error: 4.5570\n",
      "Epoch 60/100\n",
      "379/379 [==============================] - 0s 74us/step - loss: 38.3769 - mean_squared_error: 38.3769 - mean_absolute_error: 4.5135\n",
      "Epoch 61/100\n",
      "379/379 [==============================] - 0s 79us/step - loss: 36.9951 - mean_squared_error: 36.9951 - mean_absolute_error: 4.4019\n",
      "Epoch 62/100\n",
      "379/379 [==============================] - 0s 129us/step - loss: 35.8831 - mean_squared_error: 35.8831 - mean_absolute_error: 4.5219\n",
      "Epoch 63/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 36.1570 - mean_squared_error: 36.1570 - mean_absolute_error: 4.4231\n",
      "Epoch 64/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 36.3676 - mean_squared_error: 36.3676 - mean_absolute_error: 4.3834\n",
      "Epoch 65/100\n",
      "379/379 [==============================] - 0s 103us/step - loss: 34.9983 - mean_squared_error: 34.9983 - mean_absolute_error: 4.4522\n",
      "Epoch 66/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 33.7534 - mean_squared_error: 33.7534 - mean_absolute_error: 4.2494\n",
      "Epoch 67/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 33.7854 - mean_squared_error: 33.7854 - mean_absolute_error: 4.3583\n",
      "Epoch 68/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 33.2711 - mean_squared_error: 33.2711 - mean_absolute_error: 4.2163\n",
      "Epoch 69/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 32.4166 - mean_squared_error: 32.4166 - mean_absolute_error: 4.1783\n",
      "Epoch 70/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 32.2547 - mean_squared_error: 32.2547 - mean_absolute_error: 4.2589\n",
      "Epoch 71/100\n",
      "379/379 [==============================] - 0s 95us/step - loss: 32.6448 - mean_squared_error: 32.6448 - mean_absolute_error: 4.2160\n",
      "Epoch 72/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 31.6719 - mean_squared_error: 31.6719 - mean_absolute_error: 4.1676\n",
      "Epoch 73/100\n",
      "379/379 [==============================] - 0s 100us/step - loss: 30.8307 - mean_squared_error: 30.8307 - mean_absolute_error: 4.1641\n",
      "Epoch 74/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 30.3863 - mean_squared_error: 30.3863 - mean_absolute_error: 4.0326\n",
      "Epoch 75/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 31.6812 - mean_squared_error: 31.6812 - mean_absolute_error: 4.1698\n",
      "Epoch 76/100\n",
      "379/379 [==============================] - 0s 84us/step - loss: 31.0673 - mean_squared_error: 31.0673 - mean_absolute_error: 4.1733\n",
      "Epoch 77/100\n",
      "379/379 [==============================] - 0s 84us/step - loss: 29.9091 - mean_squared_error: 29.9091 - mean_absolute_error: 4.1766\n",
      "Epoch 78/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 29.8898 - mean_squared_error: 29.8898 - mean_absolute_error: 4.0385\n",
      "Epoch 79/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 28.3885 - mean_squared_error: 28.3885 - mean_absolute_error: 3.9562\n",
      "Epoch 80/100\n",
      "379/379 [==============================] - 0s 87us/step - loss: 28.2118 - mean_squared_error: 28.2118 - mean_absolute_error: 3.9587\n",
      "Epoch 81/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 29.1704 - mean_squared_error: 29.1704 - mean_absolute_error: 4.0989\n",
      "Epoch 82/100\n",
      "379/379 [==============================] - 0s 71us/step - loss: 28.1134 - mean_squared_error: 28.1134 - mean_absolute_error: 3.9748\n",
      "Epoch 83/100\n",
      "379/379 [==============================] - 0s 74us/step - loss: 28.1322 - mean_squared_error: 28.1322 - mean_absolute_error: 4.0015\n",
      "Epoch 84/100\n",
      "379/379 [==============================] - 0s 79us/step - loss: 27.3721 - mean_squared_error: 27.3721 - mean_absolute_error: 3.9170\n",
      "Epoch 85/100\n",
      "379/379 [==============================] - 0s 87us/step - loss: 27.2105 - mean_squared_error: 27.2105 - mean_absolute_error: 3.9641\n",
      "Epoch 86/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 26.6929 - mean_squared_error: 26.6929 - mean_absolute_error: 3.8909\n",
      "Epoch 87/100\n",
      "379/379 [==============================] - 0s 74us/step - loss: 26.8762 - mean_squared_error: 26.8762 - mean_absolute_error: 3.9264\n",
      "Epoch 88/100\n",
      "379/379 [==============================] - 0s 82us/step - loss: 26.9978 - mean_squared_error: 26.9978 - mean_absolute_error: 3.9104\n",
      "Epoch 89/100\n",
      "379/379 [==============================] - 0s 98us/step - loss: 26.4065 - mean_squared_error: 26.4065 - mean_absolute_error: 3.8515\n",
      "Epoch 90/100\n",
      "379/379 [==============================] - 0s 69us/step - loss: 26.5177 - mean_squared_error: 26.5177 - mean_absolute_error: 3.8820\n",
      "Epoch 91/100\n",
      "379/379 [==============================] - 0s 90us/step - loss: 25.9634 - mean_squared_error: 25.9634 - mean_absolute_error: 3.8000\n",
      "Epoch 92/100\n",
      "379/379 [==============================] - 0s 84us/step - loss: 26.0264 - mean_squared_error: 26.0264 - mean_absolute_error: 3.8140\n",
      "Epoch 93/100\n",
      "379/379 [==============================] - 0s 66us/step - loss: 25.4386 - mean_squared_error: 25.4386 - mean_absolute_error: 3.8230\n",
      "Epoch 94/100\n",
      "379/379 [==============================] - 0s 84us/step - loss: 25.1044 - mean_squared_error: 25.1044 - mean_absolute_error: 3.7674\n",
      "Epoch 95/100\n",
      "379/379 [==============================] - 0s 71us/step - loss: 24.6713 - mean_squared_error: 24.6713 - mean_absolute_error: 3.7606\n",
      "Epoch 96/100\n",
      "379/379 [==============================] - 0s 74us/step - loss: 24.9265 - mean_squared_error: 24.9265 - mean_absolute_error: 3.7492\n",
      "Epoch 97/100\n",
      "379/379 [==============================] - 0s 84us/step - loss: 23.8858 - mean_squared_error: 23.8858 - mean_absolute_error: 3.7236\n",
      "Epoch 98/100\n",
      "379/379 [==============================] - 0s 69us/step - loss: 24.7709 - mean_squared_error: 24.7709 - mean_absolute_error: 3.7463\n",
      "Epoch 99/100\n",
      "379/379 [==============================] - 0s 92us/step - loss: 24.1722 - mean_squared_error: 24.1722 - mean_absolute_error: 3.7656\n",
      "Epoch 100/100\n",
      "379/379 [==============================] - 0s 61us/step - loss: 24.1573 - mean_squared_error: 24.1573 - mean_absolute_error: 3.7054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x5f28ce4dd8>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs = 100, batch_size = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4- Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train r2: 0.73\n",
      "Test r2: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "y_test_predict = model.predict(X_test)\n",
    "\n",
    "print('Train r2: {:.2f}'.format(r2_score(y_train, y_train_predict)))\n",
    "print('Test r2: {:.2f}'.format(r2_score(y_test, y_test_predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 151.,   75.,  141.,  206.,  135.,   97.,  138.,   63.,  110.,\n",
       "        310.,  101.,   69.,  179.,  185.,  118.,  171.,  166.,  144.,\n",
       "         97.,  168.,   68.,   49.,   68.,  245.,  184.,  202.,  137.,\n",
       "         85.,  131.,  283.,  129.,   59.,  341.,   87.,   65.,  102.,\n",
       "        265.,  276.,  252.,   90.,  100.,   55.,   61.,   92.,  259.,\n",
       "         53.,  190.,  142.,   75.,  142.,  155.,  225.,   59.,  104.,\n",
       "        182.,  128.,   52.,   37.,  170.,  170.,   61.,  144.,   52.,\n",
       "        128.,   71.,  163.,  150.,   97.,  160.,  178.,   48.,  270.,\n",
       "        202.,  111.,   85.,   42.,  170.,  200.,  252.,  113.,  143.,\n",
       "         51.,   52.,  210.,   65.,  141.,   55.,  134.,   42.,  111.,\n",
       "         98.,  164.,   48.,   96.,   90.,  162.,  150.,  279.,   92.,\n",
       "         83.,  128.,  102.,  302.,  198.,   95.,   53.,  134.,  144.,\n",
       "        232.,   81.,  104.,   59.,  246.,  297.,  258.,  229.,  275.,\n",
       "        281.,  179.,  200.,  200.,  173.,  180.,   84.,  121.,  161.,\n",
       "         99.,  109.,  115.,  268.,  274.,  158.,  107.,   83.,  103.,\n",
       "        272.,   85.,  280.,  336.,  281.,  118.,  317.,  235.,   60.,\n",
       "        174.,  259.,  178.,  128.,   96.,  126.,  288.,   88.,  292.,\n",
       "         71.,  197.,  186.,   25.,   84.,   96.,  195.,   53.,  217.,\n",
       "        172.,  131.,  214.,   59.,   70.,  220.,  268.,  152.,   47.,\n",
       "         74.,  295.,  101.,  151.,  127.,  237.,  225.,   81.,  151.,\n",
       "        107.,   64.,  138.,  185.,  265.,  101.,  137.,  143.,  141.,\n",
       "         79.,  292.,  178.,   91.,  116.,   86.,  122.,   72.,  129.,\n",
       "        142.,   90.,  158.,   39.,  196.,  222.,  277.,   99.,  196.,\n",
       "        202.,  155.,   77.,  191.,   70.,   73.,   49.,   65.,  263.,\n",
       "        248.,  296.,  214.,  185.,   78.,   93.,  252.,  150.,   77.,\n",
       "        208.,   77.,  108.,  160.,   53.,  220.,  154.,  259.,   90.,\n",
       "        246.,  124.,   67.,   72.,  257.,  262.,  275.,  177.,   71.,\n",
       "         47.,  187.,  125.,   78.,   51.,  258.,  215.,  303.,  243.,\n",
       "         91.,  150.,  310.,  153.,  346.,   63.,   89.,   50.,   39.,\n",
       "        103.,  308.,  116.,  145.,   74.,   45.,  115.,  264.,   87.,\n",
       "        202.,  127.,  182.,  241.,   66.,   94.,  283.,   64.,  102.,\n",
       "        200.,  265.,   94.,  230.,  181.,  156.,  233.,   60.,  219.,\n",
       "         80.,   68.,  332.,  248.,   84.,  200.,   55.,   85.,   89.,\n",
       "         31.,  129.,   83.,  275.,   65.,  198.,  236.,  253.,  124.,\n",
       "         44.,  172.,  114.,  142.,  109.,  180.,  144.,  163.,  147.,\n",
       "         97.,  220.,  190.,  109.,  191.,  122.,  230.,  242.,  248.,\n",
       "        249.,  192.,  131.,  237.,   78.,  135.,  244.,  199.,  270.,\n",
       "        164.,   72.,   96.,  306.,   91.,  214.,   95.,  216.,  263.,\n",
       "        178.,  113.,  200.,  139.,  139.,   88.,  148.,   88.,  243.,\n",
       "         71.,   77.,  109.,  272.,   60.,   54.,  221.,   90.,  311.,\n",
       "        281.,  182.,  321.,   58.,  262.,  206.,  233.,  242.,  123.,\n",
       "        167.,   63.,  197.,   71.,  168.,  140.,  217.,  121.,  235.,\n",
       "        245.,   40.,   52.,  104.,  132.,   88.,   69.,  219.,   72.,\n",
       "        201.,  110.,   51.,  277.,   63.,  118.,   69.,  273.,  258.,\n",
       "         43.,  198.,  242.,  232.,  175.,   93.,  168.,  275.,  293.,\n",
       "        281.,   72.,  140.,  189.,  181.,  209.,  136.,  261.,  113.,\n",
       "        131.,  174.,  257.,   55.,   84.,   42.,  146.,  212.,  233.,\n",
       "         91.,  111.,  152.,  120.,   67.,  310.,   94.,  183.,   66.,\n",
       "        173.,   72.,   49.,   64.,   48.,  178.,  104.,  132.,  220.,   57.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "data = load_diabetes()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X.shape\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
